
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples\inference\plot_inference_example_lr.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__examples_inference_plot_inference_example_lr.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_inference_plot_inference_example_lr.py:


Dengue - LR
=============

.. todo:: Full explanation

.. todo:: Use column transformer.

    ctf = ColumnTransformer(
        [('gender', LabelBinarizer())]
    )

    data = ctf.fit_transform(data)

.. GENERATED FROM PYTHON SOURCE LINES 16-475



.. image:: /_examples/inference/images/sphx_glr_plot_inference_example_lr_001.png
    :alt: aucroc in HTD by month, sens in HTD by month, spec in HTD by month, prevalence in HTD by month
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Applying... oucru_dengue_interpretation_feature.


    Data rows: 11971

    by dsource:
    md       5975
    df       1661
    fl       1338
    42dx     1195
    13dx      917
    06dx      363
    01nva     221
    dr        126
    d001      110
    32dx       65
    Name: dsource, dtype: int64

    by outcome:
    0    6275
    1    5696
    Name: outcome, dtype: int64

    by sets:
    cvs    8978
    hos    2993
    Name: sets, dtype: int64


    [Pipeline] ............... (step 1 of 2) Processing std, total=   0.0s
    [Pipeline] ............... (step 2 of 2) Processing llr, total=   0.0s
    [Pipeline] ............... (step 1 of 2) Processing std, total=   0.0s
    [Pipeline] ............... (step 2 of 2) Processing llr, total=   0.0s
    [Pipeline] ............... (step 1 of 2) Processing std, total=   0.0s
    [Pipeline] ............... (step 2 of 2) Processing llr, total=   0.0s
    [Pipeline] ............... (step 1 of 2) Processing std, total=   0.0s
    [Pipeline] ............... (step 2 of 2) Processing llr, total=   0.0s
    [Pipeline] ............... (step 1 of 2) Processing std, total=   0.0s
    [Pipeline] ............... (step 2 of 2) Processing llr, total=   0.0s
    [Pipeline] ............... (step 1 of 2) Processing std, total=   0.0s
    [Pipeline] ............... (step 2 of 2) Processing llr, total=   0.0s


    Grid Search ALL scores (CVS):
                                  0
    mean_fit_time              0.02
    std_fit_time              0.001
    mean_score_time           0.003
    std_score_time              0.0
    params                       {}
    split0_test_aucroc        0.739
    split1_test_aucroc        0.727
    split2_test_aucroc        0.737
    split3_test_aucroc        0.721
    split4_test_aucroc        0.731
    mean_test_aucroc          0.731
    std_test_aucroc           0.007
    rank_test_aucroc              1
    split0_train_aucroc       0.729
    split1_train_aucroc       0.732
    split2_train_aucroc        0.73
    split3_train_aucroc       0.733
    split4_train_aucroc       0.731
    mean_train_aucroc         0.731
    std_train_aucroc          0.002
    split0_test_sensitivity   0.566
    split1_test_sensitivity   0.513
    split2_test_sensitivity   0.538
    split3_test_sensitivity   0.533
    split4_test_sensitivity   0.564
    mean_test_sensitivity     0.543
    std_test_sensitivity       0.02
    rank_test_sensitivity         1
    split0_train_sensitivity  0.539
    split1_train_sensitivity  0.546
    split2_train_sensitivity  0.541
    split3_train_sensitivity  0.554
    split4_train_sensitivity  0.549
    mean_train_sensitivity    0.546
    std_train_sensitivity     0.005
    split0_test_specificity   0.752
    split1_test_specificity   0.758
    split2_test_specificity   0.749
    split3_test_specificity   0.744
    split4_test_specificity   0.713
    mean_test_specificity     0.743
    std_test_specificity      0.016
    rank_test_specificity         1
    split0_train_specificity  0.743
    split1_train_specificity   0.75
    split2_train_specificity   0.75
    split3_train_specificity  0.739
    split4_train_specificity  0.743
    mean_train_specificity    0.745
    std_train_specificity     0.004

    Grid Search MEAN scores (CVS):
                                0
    mean_fit_time           0.020
    mean_score_time         0.003
    mean_test_aucroc        0.731
    mean_test_sensitivity   0.543
    mean_test_specificity   0.743
    mean_train_aucroc       0.731
    mean_train_sensitivity  0.546
    mean_train_specificity  0.745

    Best estimator HOS scores:
    AUCROC         0.736
    Sensitivity    0.545
    Specificity    0.737
    dtype: float64
           aucroc    sens    spec  prevalence     n
    month                                          
    1      75.542  57.862  76.471      44.852   709
    2      69.342  50.833  75.694      29.412   408
    3      75.402  58.020  75.495      42.037   697
    4      66.005  41.520  73.575      30.700   557
    5      70.564  55.000  73.378      32.984   667
    6      76.858  58.779  78.000      37.680  1043
    7      73.455  53.344  78.718      46.730  1376
    8      76.389  54.178  76.990      56.212  1320
    9      76.209  54.208  76.091      55.024  1274
    10     69.082  54.768  66.893      56.850  1365
    11     70.102  53.243  67.732      50.227  1320
    12     72.743  54.726  73.402      53.117  1235

    Monthly scores:
         aucroc    sens    spec  prevalence     n
    Jan  75.542  57.862  76.471      44.852   709
    Feb  69.342  50.833  75.694      29.412   408
    Mar  75.402  58.020  75.495      42.037   697
    Apr  66.005  41.520  73.575      30.700   557
    May  70.564  55.000  73.378      32.984   667
    Jun  76.858  58.779  78.000      37.680  1043
    Jul  73.455  53.344  78.718      46.730  1376
    Aug  76.389  54.178  76.990      56.212  1320
    Sep  76.209  54.208  76.091      55.024  1274
    Oct  69.082  54.768  66.893      56.850  1365
    Nov  70.102  53.243  67.732      50.227  1320
    Dec  72.743  54.726  73.402      53.117  1235

    Monthly scores (stacked):
       month      metric    result
    0    Jan      aucroc    75.542
    1    Jan        sens    57.862
    2    Jan        spec    76.471
    3    Jan  prevalence    44.852
    4    Jan           n   709.000
    5    Feb      aucroc    69.342
    6    Feb        sens    50.833
    7    Feb        spec    75.694
    8    Feb  prevalence    29.412
    9    Feb           n   408.000
    10   Mar      aucroc    75.402
    11   Mar        sens    58.020
    12   Mar        spec    75.495
    13   Mar  prevalence    42.037
    14   Mar           n   697.000
    15   Apr      aucroc    66.005
    16   Apr        sens    41.520
    17   Apr        spec    73.575
    18   Apr  prevalence    30.700
    19   Apr           n   557.000
    20   May      aucroc    70.564
    21   May        sens    55.000
    22   May        spec    73.378
    23   May  prevalence    32.984
    24   May           n   667.000
    25   Jun      aucroc    76.858
    26   Jun        sens    58.779
    27   Jun        spec    78.000
    28   Jun  prevalence    37.680
    29   Jun           n  1043.000
    30   Jul      aucroc    73.455
    31   Jul        sens    53.344
    32   Jul        spec    78.718
    33   Jul  prevalence    46.730
    34   Jul           n  1376.000
    35   Aug      aucroc    76.389
    36   Aug        sens    54.178
    37   Aug        spec    76.990
    38   Aug  prevalence    56.212
    39   Aug           n  1320.000
    40   Sep      aucroc    76.209
    41   Sep        sens    54.208
    42   Sep        spec    76.091
    43   Sep  prevalence    55.024
    44   Sep           n  1274.000
    45   Oct      aucroc    69.082
    46   Oct        sens    54.768
    47   Oct        spec    66.893
    48   Oct  prevalence    56.850
    49   Oct           n  1365.000
    50   Nov      aucroc    70.102
    51   Nov        sens    53.243
    52   Nov        spec    67.732
    53   Nov  prevalence    50.227
    54   Nov           n  1320.000
    55   Dec      aucroc    72.743
    56   Dec        sens    54.726
    57   Dec        spec    73.402
    58   Dec  prevalence    53.117
    59   Dec           n  1235.000

    Correlation with prevalence (pearson):
                prevalence         n
    aucroc        0.396228  0.315377
    sens          0.364747  0.279029
    spec         -0.254064 -0.204732
    prevalence    1.000000  0.873310
    n             0.873310  1.000000

    Correlation with prevalence (spearman):
                prevalence         n
    aucroc        0.244755  0.227671
    sens          0.090909 -0.007005
    spec         -0.104895  0.105079
    prevalence    1.000000  0.844134
    n             0.844134  1.000000






|

.. code-block:: default
   :lineno-start: 18



    # Libraries
    import os
    import sys
    import calendar
    import numpy as np
    import pandas as pd
    #import modin.pandas as pd

    # Libraries imblearn
    from imblearn.pipeline import Pipeline
    from imblearn.over_sampling import SMOTE

    # Libraries sklearn
    from sklearn.compose import ColumnTransformer
    from sklearn.model_selection import train_test_split
    from sklearn.model_selection import StratifiedKFold
    from sklearn.model_selection import GridSearchCV
    from sklearn.preprocessing import StandardScaler
    from sklearn.preprocessing import LabelBinarizer
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import make_scorer
    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import recall_score
    from sklearn.metrics import roc_auc_score

    # Import DataBlend
    from datablend.core.repair.correctors import oucru_dengue_interpretation_feature
    from datablend.core.repair.correctors import static_correction

    # ---------------------------------
    # Methods
    # ---------------------------------
    def cvs_hos_split(data, **kwargs):
        """This method labels the dataframe hos and cvs sets.

        Parameters
        ----------
        data: np.array or pd.DataFrame
            The data to be divided into HOS/CVS.

        Returns
        -------
        np.array:
            The outcome is a numpy array with rows labelled as
            cvs (cross-validation set) and hos (hold-out set).
        """
        # Length
        r, c = data.shape

        # Create indexes to use for splitting
        idxs = np.arange(r).reshape(-1, 1)

        # Split in hos and training sets
        cvs, hos = train_test_split(idxs, **kwargs)

        # Create result
        empty = np.array([None] * data.shape[0])
        empty[cvs] = 'cvs'
        empty[hos] = 'hos'

        # Convert to series
        if isinstance(data, pd.DataFrame):
            empty = pd.Series(index=data.index, data=empty)

        # Return
        return empty

    # .. note: This is computationally expensive because it
    #          calls the same method (confusion_matrix) four
    #          times. But scorer functions need to be created
    #          for GridSearchCV.

    def tp(y, y_pred, **kwargs):
        """Return the true positives"""
        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()
        return tp

    def tn(y, y_pred, **kwargs):
        """Return the true negatives"""
        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()
        return tn

    def fp(y, y_pred, **kwargs):
        """Return the false positives"""
        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()
        return fp

    def fn(y, y_pred, **kwargs):
        """Return the false negatives"""
        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()
        return fn


    # ---------------------------------
    # Constants
    # ---------------------------------
    # The data filepath
    path = '../../resources/data/20210313-v0.0.8/'
    path+= 'combined/combined_tidy.csv'

    # Features
    features = sorted(['age',
                       #'gender',
                       'plt',
                       'haematocrit_percent'])

    # Usecols
    usecols = ['date', 'study_no', 'dsource',
        'day_from_admission', 'pcr_dengue_serotype',
        'pcr_dengue_load', 'ns1_interpretation',
        'igm_interpretation', 'igg_interpretation',
        'serology_single_interpretation',
        'serology_paired_interpretation']
    usecols+= features

    # ---------------------------------
    # Main
    # ---------------------------------

    # ---------
    # Load data
    # ---------
    # Read data
    data = pd.read_csv(path,
        low_memory=False,
        #nrows=50000,
        usecols=usecols,
        parse_dates=['date'])

    # Reset index
    data = data.reset_index()

    # Remove columns with all NaN
    data = data.dropna(how='all', axis=1)
    data = data.dropna(how='any', subset=features)

    # Add dengue interpretation
    data['dengue_interpretation'] = \
        oucru_dengue_interpretation_feature(data,
            pcr=True, ns1=False, igm=False, serology=False,
            single_igm_igg=False, paired_igm_igg=False,
            default=False)

    # Create outcome
    data['outcome'] = \
        data.dengue_interpretation.astype(int)

    # Add month
    data['month'] = data.date.dt.month

    # -----------
    # Format data
    # -----------

    # Keep only day_from_admission == 0 data
    data = data[data.day_from_admission.isin([0, 1])]

    # Keep only certain months
    #data = data[data.month.isin([2, 3, 4])]

    # Drop any without interpretation
    data = data[~data.dengue_interpretation.isna()]

    # Do this with sklearn column transformer
    if 'gender' in data:
        data.gender = data.gender.replace({'Male': 0, 'Female': 1})

    # -----------
    # Add splits
    # -----------
    data['sets'] = cvs_hos_split(data, stratify=data.outcome)

    # Show information
    print("\n\nData rows: %s" % data.shape[0])
    print("\nby dsource:")
    print(data.dsource.value_counts())
    print("\nby outcome:")
    print(data.outcome.value_counts(dropna='False'))
    print("\nby sets:")
    print(data.sets.value_counts(dropna='False'))
    print("\n")


    # ----------
    # Train
    # ----------
    # Estimator
    estimator = LogisticRegression()

    param_grid = {}

    # Scoring
    scoring = {
        'aucroc': 'roc_auc',
        'sensitivity': make_scorer(recall_score),
        'specificity': make_scorer(recall_score, pos_label=0),
        #'tp': make_scorer(tp),
        #'tn': make_scorer(tn),
        #'fp': make_scorer(fp),
        #'fn': make_scorer(fn),
    }

    # Create pipeline
    pipe = Pipeline(steps=[#('smt', SMOTE(random_state=42)),
                           ('std', StandardScaler()),
                           ('llr', estimator)],
                    memory='./outputs',
                    verbose=True)

    # Create k-folds
    kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)

    # Create grid search
    grid = GridSearchCV(pipe, cv=kf, scoring=scoring,
        param_grid=param_grid, return_train_score=True,
        refit='aucroc')

    # Define
    features = features
    target = 'outcome'

    # Get train data
    train = data[data.sets == 'cvs']
    train = train[features + [target]]
    #train = train.dropna(how='any') # done already

    # Get X and y
    X = train[features].to_numpy()
    y = train[target].to_numpy()

    # Fit
    grid.fit(X, y);

    # ----------------
    # Show scores
    # ----------------
    # Show results
    results = pd.DataFrame(grid.cv_results_)

    #print("\nResult columns:")
    #print(results.columns.values)
    print("\n\nGrid Search ALL scores (CVS):")
    print(results.round(decimals=3).T)

    # Select columns
    columns = [c for c in results.columns if 'param_' in c]
    columns+= [c for c in results.columns if 'mean_' in c]
    columns = sorted(columns)

    # Show
    print("\nGrid Search MEAN scores (CVS):")
    print(results[columns].round(decimals=3).T)

    # -------------------
    # Best estimator
    # -------------------
    # .. note: It is possible to evaluate the estimator
    #          only in HOS. In addition, we would like to
    #          see how the predictions and metrics vary
    #          according to the seasonality.
    # Get best estimator
    best_estimator = grid.best_estimator_

    # -----------------
    # Evaluation in HOS
    # -----------------
    # Evaluate best estimator on HOS
    hos = data[data.sets == 'hos']

    # Get X and y
    X = hos[features].to_numpy()
    y = hos[target].to_numpy()

    # Do predictions
    y_pred = best_estimator.predict(X)
    y_score = best_estimator.predict_proba(X)

    # Scores
    aucroc = roc_auc_score(y, y_score[:, 1])
    sensitivity = recall_score(y, y_pred)
    specificity = recall_score(y, y_pred, pos_label=0)

    # Create series
    series = pd.Series(\
        index=['AUCROC', 'Sensitivity', 'Specificity'],
        data=[aucroc, sensitivity, specificity])

    # Show
    print("\nBest estimator HOS scores:")
    print(series.round(decimals=3))

    # --------------------------------
    # Evaluation of seasonality effect
    # --------------------------------
    # Get X and y
    X = data[features].to_numpy()
    y = data[target].to_numpy()

    # Add columns to data
    data['y_pred'] = best_estimator.predict(X)
    data['y_score'] = best_estimator.predict_proba(X)[:, 1]

    #
    def _aucroc(x):
        try:
            return roc_auc_score(x.outcome, x.y_score)
        except:
            return None

    def _sens(x):
        try:
            return recall_score(x.outcome, x.y_pred)
        except:
            return None

    def _spec(x):
        try:
            return recall_score(x.outcome, x.y_pred, pos_label=0)
        except:
            return None

    def prevalence(x):
        return (np.sum(x) / len(x)) * 100

    # Outputs
    aucroc = data.groupby('month').apply(_aucroc)
    sensitivity = data.groupby('month').apply(_sens)
    specificity = data.groupby('month').apply(_spec)
    prevalence = data.groupby('month').outcome.apply(prevalence)

    # Scores
    scores = pd.DataFrame()
    scores['aucroc'] = aucroc * 100
    scores['sens'] = sensitivity * 100
    scores['spec'] = specificity * 100
    scores['prevalence'] = prevalence
    scores['n'] = data.groupby('month').study_no.count()
    #scores['sens_prev_ratio'] = scores.sens / scores.prevalence
    #scores['spec_prev_ratio'] = scores.spec / scores.prevalence
    #scores['spec_prev_ratio'] = scores.spec / scores.prevalence
    #scores['spec_n_ratio'] = scores.spec / scores.n
    scores = scores.round(decimals=3)

    print(scores)

    # Add legible labels
    scores.index = \
         [calendar.month_abbr[x] for x in scores.index]

    # Create stacked version (seaborn)
    scores_stacked = scores.stack().reset_index()
    scores_stacked.columns = ['month', 'metric', 'result']

    # Show
    print("\nMonthly scores:")
    print(scores)

    print("\nMonthly scores (stacked):")
    print(scores_stacked)

    print("\nCorrelation with prevalence (pearson):")
    print(scores.corr()[['prevalence', 'n']])
    print("\nCorrelation with prevalence (spearman):")
    print(scores.corr(method='spearman')[['prevalence', 'n']])

    # ------------------------------------------------
    # Plot
    # ------------------------------------------------
    # Libraries
    import seaborn as sns
    import matplotlib.pyplot as plt

    # Seaborn
    sns.set_theme(style="whitegrid")


    # --------------
    # Plot FacetGrid
    # --------------
    """
    sns.set_color_codes("muted")
    sns.despine(left=True, bottom=True)

    # Create facet grid
    g = sns.FacetGrid(scores_stacked,
        col="metric", col_wrap=3, sharey=False,
        aspect=1.5)

    # Plot sns plots
    g.map_dataframe(sns.barplot, x="month",
        y="result", linewidth=0.76)
    """

    # ---------------
    # Plot main
    # ---------------
    # Colors
    colors = ['b', 'g', 'r', 'b']

    # Initialize the figure
    f, axes = plt.subplots(1, 4, figsize=(12, 2.5), sharey=True)
    axes = axes.flatten()

    # Plot
    for i, c in enumerate(['aucroc', 'sens', 'spec', 'prevalence']):

        # Plot barplot
        sns.despine(left=True, bottom=True)
        sns.barplot(x=scores.index,
                 y=scores[c],
                 label=c,
                 color=colors[i],
                 ax=axes[i],
                 linewidth=0.75)

        # Add a legend and informative axis label
        axes[i].legend(ncol=2, loc="lower right", frameon=True)
        axes[i].set(xlabel="", ylabel="",
            title="%s in HTD by month" % c)

    # Tight layout
    plt.tight_layout()


    """
    # --------------------
    # Plot others
    # --------------------
    # Initialize the figure
    f, axes = plt.subplots(1, 4, figsize=(12, 2.5))
    axes = axes.flatten()

    # Plot
    for i, c in enumerate(['aucroc',
                           'sens',
                           'spec']):

        # Plot barplot
        sns.despine(left=True, bottom=True)
        sns.barplot(x=scores.index,
                 y=scores[c] / scores.prevalence,
                 label=c,
                 color='b',
                 ax=axes[i],
                 linewidth=0.75)

        # Add a legend and informative axis label
        axes[i].legend(ncol=2, loc="lower right", frameon=True)
        axes[i].set(xlabel="", ylabel="",
            title="%s in HTD by month" % c)

    plt.tight_layout()
    """

    plt.show()


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  2.554 seconds)


.. _sphx_glr_download__examples_inference_plot_inference_example_lr.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_inference_example_lr.py <plot_inference_example_lr.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_inference_example_lr.ipynb <plot_inference_example_lr.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
